{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T03:51:31.764930400Z",
     "start_time": "2023-09-24T03:51:31.706088Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: cannot import name '_CopyMode' from 'numpy._globals' (D:\\Anaconda\\lib\\site-packages\\numpy\\_globals.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\__init__.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         missing_dependencies\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdependency\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_dependencies:\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to import required dependencies:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_dependencies)\n\u001b[0;32m     18\u001b[0m     )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m hard_dependencies, dependency, missing_dependencies\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to import required dependencies:\nnumpy: cannot import name '_CopyMode' from 'numpy._globals' (D:\\Anaconda\\lib\\site-packages\\numpy\\_globals.py)"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import random as rd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T03:51:40.882696700Z",
     "start_time": "2023-09-24T03:51:40.845116200Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yaml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n\u001b[1;32m----> 5\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mread_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigs.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mread_yaml\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_yaml\u001b[39m(file_path):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 3\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43myaml\u001b[49m\u001b[38;5;241m.\u001b[39msafe_load(f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yaml' is not defined"
     ]
    }
   ],
   "source": [
    "def read_yaml(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "ds = read_yaml(file_path = \"configs.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_config = \"MovieLens100K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(f\"datasets/{ds[ds_config]['ds_name']}/{ds[ds_config]['ds_name']}_train.txt\", sep='\\t', names=['uid', 'iid', 'rating'])\n",
    "test_data = pd.read_csv(f\"datasets/{ds[ds_config]['ds_name']}/{ds[ds_config]['ds_name']}_test.txt\", sep='\\t', names=['uid', 'iid', 'rating'])\n",
    "long_tail_items = pd.read_csv(f\"datasets/{ds[ds_config]['ds_name']}/groups/items/020/longtail_items.txt\", names=['iid'])\n",
    "short_head_items = pd.read_csv(f\"datasets/{ds[ds_config]['ds_name']}/groups/items/020/shorthead_items.txt\", names=['iid'])\n",
    "active_users = pd.read_csv(f\"datasets/{ds[ds_config]['ds_name']}/groups/users/005/active_ids.txt\", names=['uid'])\n",
    "inactive_users = pd.read_csv(f\"datasets/{ds[ds_config]['ds_name']}/groups/users/005/inactive_ids.txt\", names=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ids = train_data['uid'].unique()\n",
    "items_ids = train_data['iid'].unique()\n",
    "num_users = len(train_data['uid'].unique())\n",
    "num_items = len(train_data['iid'].unique())\n",
    "active_users_list = list(active_users['uid'].unique())\n",
    "inactive_users_list = list(inactive_users['uid'].unique())\n",
    "longtail_items_list = list(long_tail_items['iid'].unique())\n",
    "shorthead_items_list = list(short_head_items['iid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_items = dict()\n",
    "\n",
    "for eachline in train_data.iterrows():\n",
    "    uid, iid = eachline[1][0], eachline[1][0]\n",
    "    uid, iid = int(uid), int(iid)\n",
    "    # a dictionary of popularity of items\n",
    "    if iid in pop_items.keys():\n",
    "      pop_items[iid] += 1\n",
    "    else:\n",
    "      pop_items[iid] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precisionk(actual, predicted):\n",
    "  return 1.0 * len(set(actual) & set(predicted)) / len(predicted)\n",
    "\n",
    "def recallk(actual, predicted):\n",
    "  return 1.0 * len(set(actual) & set(predicted)) / len(actual)\n",
    "\n",
    "def ndcgk(actual, predicted):\n",
    "  idcg = 1.0\n",
    "  dcg = 1.0 if predicted[0] in actual else 0.0\n",
    "  for i, p in enumerate(predicted[1:]):\n",
    "    if p in actual:\n",
    "      dcg += 1.0 / np.log(i+2)\n",
    "    idcg += 1.0 / np.log(i+2)\n",
    "  return dcg / idcg\n",
    "\n",
    "def novelty(predicted: list, pop: dict, u: int, k: int) -> float:\n",
    "  self_information = 0\n",
    "  for item in predicted:\n",
    "    if item in pop.keys():\n",
    "      item_popularity = pop[item] / u\n",
    "      item_novelty_value = np.sum(-np.log2(item_popularity))\n",
    "    else:\n",
    "      item_novelty_value = 0\n",
    "    self_information += item_novelty_value\n",
    "  novelty_score = self_information / k\n",
    "  return novelty_score\n",
    "\n",
    "def catalog_coverage(predicted: list, catalog: list) -> float:\n",
    "  predicted_flattened = [p for sublist in predicted for p in sublist]\n",
    "  L_predictions = len(set(predicted_flattened))\n",
    "  catalog_coverage = round(L_predictions / (len(catalog) * 1.0), 2)\n",
    "  return catalog_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data\n",
    "def read_ground_truth():\n",
    "  ground_truth = defaultdict(set)\n",
    "  for eachline in test_data.iterrows():\n",
    "    uid, iid = eachline[1][0], eachline[1][1]\n",
    "    uid, iid = int(uid), int(iid)\n",
    "    ground_truth[uid].add(iid)\n",
    "  return ground_truth\n",
    "\n",
    "ground_truth = read_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_per_group(group):\n",
    "  # group: a list of users ids\n",
    "\n",
    "  NDCG10 = list()\n",
    "  Pre10 = list()\n",
    "  Rec10 = list()\n",
    "  Novelty10 = list()\n",
    "  predicted = list()\n",
    "  All_Predicted = list()\n",
    "\n",
    "  long_items = 0\n",
    "  short_items = 0\n",
    "  all_items = 0\n",
    "\n",
    "  for uid in tqdm(group):\n",
    "    if uid in ground_truth.keys():\n",
    "      # print(uid)\n",
    "      for item in top_n_random[uid]:\n",
    "        all_items += 1\n",
    "        if item in longtail_items_list:\n",
    "          long_items += 1\n",
    "        if item in shorthead_items_list:\n",
    "          short_items += 1\n",
    "        predicted.append(item)\n",
    "\n",
    "      copy_predicted = predicted[:] \n",
    "      All_Predicted.append(copy_predicted)\n",
    "      NDCG = ndcgk(actual=ground_truth[uid], predicted=predicted)\n",
    "      Pre = precisionk(actual=ground_truth[uid], predicted=predicted)\n",
    "      Rec = recallk(actual=ground_truth[uid], predicted=predicted)\n",
    "      Novelty = novelty(predicted=predicted, pop=pop_items, u=num_users, k=10)\n",
    "\n",
    "      NDCG10.append(NDCG)\n",
    "      Pre10.append(Pre)\n",
    "      Rec10.append(Rec)\n",
    "      Novelty10.append(Novelty)\n",
    "    \n",
    "      # cleaning the predicted list for a new user\n",
    "      predicted.clear()\n",
    "\n",
    "  catalog = catalog_coverage(predicted=All_Predicted, catalog=pop_items.keys())\n",
    "  return round(np.mean(NDCG10), 5), round(np.mean(Pre10), 5), round(np.mean(Rec10), 5), round(np.mean(Novelty10), 5), catalog, short_items, long_items, all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random model is selected:\n"
     ]
    }
   ],
   "source": [
    "def get_top_n_random(n=10):\n",
    "  print(\"Random model is selected:\")\n",
    "  top_n = defaultdict(list)\n",
    "  items = list(items_ids)\n",
    "  for uid in users_ids:\n",
    "    if uid not in top_n.keys():\n",
    "      # user_id = list(rs.train_set.user_ids)[uid]\n",
    "      for i in range(0, n):\n",
    "        top_n[int(uid)].append(int(rd.choice(items)))\n",
    "  return top_n\n",
    "\n",
    "top_n_random = get_top_n_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 512.12it/s]\n",
      "100%|██████████| 896/896 [00:01<00:00, 514.00it/s]\n",
      "100%|██████████| 943/943 [00:01<00:00, 517.70it/s]\n"
     ]
    }
   ],
   "source": [
    "result_file = open(f\"{ds_config}_random.txt\", \"w\")\n",
    "for groups in [('active', active_users_list), ('inactive', inactive_users_list), ('all', users_ids)]:\n",
    "    results = metric_per_group(group=groups[1])\n",
    "    result_file.write(f\"{groups[0]}: {results}\\n\")\n",
    "result_file.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
